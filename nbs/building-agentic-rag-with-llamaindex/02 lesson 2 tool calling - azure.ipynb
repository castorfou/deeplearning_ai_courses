{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_azure_openai_keys\n",
    "\n",
    "AZURE_API_KEY, AZURE_ENDPOINT, AZURE_API_VERSION = get_azure_openai_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Adds two integers together.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "\n",
    "def mystery(x: int, y: int) -> int:\n",
    "    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n",
    "    return (x + y) * (x + y)\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "mystery_tool = FunctionTool.from_defaults(fn=mystery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: mystery with args: {\"x\": 2, \"y\": 9}\n",
      "=== Function Output ===\n",
      "121\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "from helper import get_azure_llm\n",
    "\n",
    "llm = get_azure_llm()\n",
    "response = llm.predict_and_call(\n",
    "    [add_tool, mystery_tool],\n",
    "    \"Tell me the output of the mystery function on 2 and 9\",\n",
    "    verbose=True,\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# auto retrieval tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_azure_llm, get_azure_embed_model\n",
    "\n",
    "llm = get_azure_llm()\n",
    "embed_model = get_azure_embed_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from helper import get_data_path\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[get_data_path(\"metagpt.pdf\")]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 25\n",
      "file_name: metagpt.pdf\n",
      "file_path: /mnt/c/Users/f279814/git/deeplearning_ai_courses/data/building-agentic-rag-with-llamaindex/metagpt.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 16911937\n",
      "creation_date: 2025-01-03\n",
      "last_modified_date: 2025-01-03\n",
      "\n",
      "Preprint\n",
      "be given a function signature and its docstring by the user. Write your full implementation (restate\n",
      "the function signature).” As shown in Table 7, GPT-4 is more sensitive to prompt, code parser, and\n",
      "post-processing results on the HumanEval data set. It is difficult for GPT-3.5-Turbo to return the\n",
      "correct completion code without prompt words.\n",
      "Table 7: Performance of GPT models on HumanEval. Experiments were conducted five times\n",
      "using gpt-4-0613 and gpt-3.5-turbo-0613 with different settings.\n",
      "Settings Model 1 2 3 4 5 Avg. Std.\n",
      "A gpt-4-0613 0.732 0.707 0.732 0.713 0.738 0.724 0.013\n",
      "A gpt-3.5-turbo-0613 0.360 0.366 0.360 0.348 0.354 0.357 0.007\n",
      "B gpt-4-0613 0.787 0.811 0.817 0.829 0.817 0.812 0.016\n",
      "B gpt-3.5-turbo-0613 0.348 0.354 0.348 0.335 0.348 0.346 0.007\n",
      "C gpt-4-0613 0.805 0.805 0.817 0.793 0.780 0.800 0.014\n",
      "C gpt-3.5-turbo-0613 0.585 0.567 0.573 0.579 0.579 0.577 0.007\n",
      "Qualitative results Figure 11 and Figure 12 illustrate the outcomes of the Architect agent’s ef-\n",
      "forts to design a complex recommender system. These figures showcase the comprehensive system\n",
      "interface design and program call flow. The latter is essential for creating a sophisticated automated\n",
      "system. It is crucial to emphasize the importance of this division of labor in developing an automated\n",
      "software framework.\n",
      "D L IMITATION AND ETHICS CONCERNS\n",
      "D.1 L IMITATION\n",
      "System side At present, our system cannot fully cater to specific scenarios, such as UI and front-\n",
      "end, as we have yet to incorporate such agents and multimodal tools. Furthermore, despite gen-\n",
      "erating the most amount of code among comparable frameworks, it remains challenging to fulfill\n",
      "real-world applications’ diverse and complex requirements.\n",
      "Human user side A key challenge for users is to interrupt the running process of each agent, or\n",
      "set the starting running point (checkpoint) for each agent.\n",
      "D.2 E THICS CONCERNS\n",
      "Unemployment and Skill Obsolescence MetaGPT enables more people to program in natural\n",
      "languages, thereby making it easier for engineers to get started. Over the years, programming\n",
      "languages have evolved from punched cards to assembly, C, Java, Python, and now natural lan-\n",
      "guage. As a result, humans have become more proficient at programming, increasing the demand\n",
      "for programming-related positions. Furthermore, programming with natural language may offer a\n",
      "significantly easier learning curve, making programming more accessible to a broader audience.\n",
      "Transparency and Accountability MetaGPT is an open-source framework that facilitates inter-\n",
      "active communication between multiple agents through natural language. Humans can initiate, ob-\n",
      "serve, and stop running with the highest level of control. It provides real-time interpretation and op-\n",
      "eration of the natural language, displayed on the screen and logs, ensuring transparency. MetaGPT\n",
      "enhances “natural language programming” capabilities, and human engineers are the users and re-\n",
      "sponsible for the outcomes.\n",
      "Privacy and Data Security MetaGPT operates locally, ensuring user data privacy and security. It\n",
      "does not collect user data. For interactions with third-party LLMs, such as those by OpenAI, users\n",
      "are encouraged to refer to the respective privacy policies (e.g., OpenAI Privacy Policy). However,\n",
      "we provide the option of open-source LLMs as backends.\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(nodes[30].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 page_label: 1\n",
      "\n",
      "1 page_label: 2\n",
      "\n",
      "2 page_label: 3\n",
      "\n",
      "3 page_label: 3\n",
      "\n",
      "4 page_label: 4\n",
      "\n",
      "5 page_label: 5\n",
      "\n",
      "6 page_label: 6\n",
      "\n",
      "7 page_label: 7\n",
      "\n",
      "8 page_label: 7\n",
      "\n",
      "9 page_label: 8\n",
      "\n",
      "10 page_label: 9\n",
      "\n",
      "11 page_label: 10\n",
      "12 page_label: 10\n",
      "13 page_label: 11\n",
      "14 page_label: 11\n",
      "15 page_label: 12\n",
      "16 page_label: 12\n",
      "17 page_label: 13\n",
      "18 page_label: 13\n",
      "19 page_label: 14\n",
      "20 page_label: 15\n",
      "21 page_label: 16\n",
      "22 page_label: 17\n",
      "23 page_label: 18\n",
      "24 page_label: 19\n",
      "25 page_label: 20\n",
      "26 page_label: 21\n",
      "27 page_label: 22\n",
      "28 page_label: 23\n",
      "29 page_label: 24\n",
      "30 page_label: 25\n",
      "31 page_label: 26\n",
      "32 page_label: 27\n",
      "33 page_label: 28\n",
      "34 page_label: 29\n"
     ]
    }
   ],
   "source": [
    "for i, node in enumerate(nodes):\n",
    "    print(f\"{i} {node.get_metadata_str(mode='embed')[:14]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    filters=MetadataFilters.from_dicts([{\"key\": \"page_label\", \"value\": \"2\"}]),\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"What are some high-level results of MetaGPT?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetaGPT achieves a new state-of-the-art with 85.9% and 87.7% in Pass@1 on code generation benchmarks. It also demonstrates a 100% task completion rate in experimental evaluations, showcasing its robustness and efficiency in handling complex software projects.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': '/mnt/c/Users/f279814/git/deeplearning_ai_courses/data/building-agentic-rag-with-llamaindex/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-03', 'last_modified_date': '2025-01-03'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define auto retrieval tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "\n",
    "\n",
    "def vector_query(query: str, page_numbers: List[str]) -> str:\n",
    "    \"\"\"Perform a vector search over an index.\n",
    "\n",
    "    query (str): the string query to be embedded.\n",
    "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n",
    "        over all pages. Otherwise, filter by the set of specified pages.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [{\"key\": \"page_label\", \"value\": p} for p in page_numbers]\n",
    "\n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts, condition=FilterCondition.OR\n",
    "        ),\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "\n",
    "\n",
    "vector_query_tool = FunctionTool.from_defaults(name=\"vector_tool\", fn=vector_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"high-level results of MetaGPT\", \"page_numbers\": [\"2\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT achieves a new state-of-the-art with 85.9% and 87.7% in Pass@1 for code generation benchmarks. It also demonstrates a 100% task completion rate, showcasing its robustness and efficiency in handling complex software projects.\n"
     ]
    }
   ],
   "source": [
    "llm = get_azure_llm()\n",
    "\n",
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool],\n",
    "    \"What are the high-level results of MetaGPT as described on page 2?\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': '/mnt/c/Users/f279814/git/deeplearning_ai_courses/data/building-agentic-rag-with-llamaindex/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-03', 'last_modified_date': '2025-01-03'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# and with other tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    name=\"summary_tool\",\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\"Useful if you want to get a summary of MetaGPT\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"MetaGPT comparisons with ChatDev\", \"page_numbers\": [\"8\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT outperforms ChatDev in several key metrics on the SoftwareDev dataset. MetaGPT achieves a higher executability score of 3.75 compared to ChatDev's 2.25. It also takes less time to run (503 seconds versus 762 seconds) and requires fewer tokens to generate one line of code (126.5/124.3 tokens compared to ChatDev's 248.9 tokens). Additionally, MetaGPT produces more code files, more lines of code per file, and a higher total number of code lines. The cost of human revision is also significantly lower for MetaGPT (0.83) compared to ChatDev (2.5).\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool],\n",
    "    \"What are the MetaGPT comparisons with ChatDev described on page 8?\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '8', 'file_name': 'metagpt.pdf', 'file_path': '/mnt/c/Users/f279814/git/deeplearning_ai_courses/data/building-agentic-rag-with-llamaindex/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-03', 'last_modified_date': '2025-01-03'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"MetaGPT\"}\n",
      "=== Function Output ===\n",
      "MetaGPT is a meta-programming framework designed to enhance software development through multi-agent collaboration based on large language models (LLMs). It assigns specific roles to agents, such as Product Manager, Architect, Engineer, and QA Engineer, to streamline workflows and reduce errors. The framework emphasizes structured communication through documents and diagrams, incorporates Standardized Operating Procedures (SOPs), and uses an executable feedback mechanism for iterative code improvement. MetaGPT has demonstrated superior performance in benchmarks like HumanEval and MBPP, excelling in metrics such as executability, cost efficiency, and productivity. It also supports continuous learning and improvement by allowing agents to modify their constraint prompts based on previous feedback. Additionally, MetaGPT operates locally to ensure data privacy and security and can interact with third-party LLMs while providing options for open-source LLMs as backends.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool], \"What is a summary of the paper?\", verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
